
# Project 1

Below are summaries and compile commands for the algorithms implemented.

## A-Priori

The A-Priori program implements the algorithm efficiently, and avoids keeping infrequent items (and their counts)
between passes. By transforming the item counts from Pass 1 into a set of frequent items, memory is saved, and
is sufficient input to Pass 2. I have implemented both the Triangular Matrix / Array (approach 1) and Count Triples
(approach 2) in this file. You will see results for both in the output. You would obviously pick *one* in a real application,
however because the input size is manageable, *I have chosen to illustrate both*.

`g++-7 -std=c++17 -o apriori a_priori.cxx`
`./apriori retail.txt <thresh eg. 10 or 5>`

## PCY

The PCY algorithm implements the multistage extension to the traditional PCY algorithm, finishing in 3 passes. For
efficiency, hash tables are stored temporarily, as vectors. Before finishing a pass, each pass performs its respective
between-class duties. As such, the hash table is removed from memory and transformed / returned as a bitmap,
representing the support frequency of a bucket at the given indices. After testing, 100,000 buckets was determined to
be a sufficient number of buckets to account for in the hashing algorithm.

`g++-7 -std=c++17 -o pcy pcy.cxx`
`./pcy retail.txt <thresh eg. 10 or 5>`

## Scalability Study

The scalability study performs a comparison of the performance of the A-Priori and PCY algorithms. The comparison metric
is runtime, when run over identical subsets of the original dataset (retail.txt). The results are illustrated in the images provided
in the *results* directory, run using support thresholds of 1%, 5% and 10%. Over many runs of this study, it is clear that, using
the support thresholds tested, that the *Multistage PCY algorithm requires a significantly longer period of time to complete in
comparison with A-Priori*, with the severity increasing linearly (PCY much faster than A-Priori) with larger dataset sizes.

`g++-7 -std=c++17 -o study scalability_study.cxx -lboost_iostreams -lboost_system -lboost_filesystem`
`./study retail.txt a_priori.cxx pcy.cxx`

## Results

The results of the frequent itemset / pairs mining are found in the *results/* directory. Inside, there is a directory for each
frequency algorithm used. Each contains space delimited *.txt* files, with each representing the output for that algorithm
run on a given percentage of the input data, and a given support threshold. They are named specifically to specify these
parameters. In the files, the first two columns are the pairs, the third is the occurrence count *from the given input percentage*.
The results are generated by calling the *write_pairs* function, currently commented out, in the last pass of each
algorithm. For PCY, the output is generated by fixing the number of baskets to the predetermined sufficient value of 100,000.


### Dependencies

- gcc 7.2.0
- boost 1.66.0
- gnuplot 5.2 (separate header for C++ use is included)

### System Specs
- Hardware:  MacBook Pro (Retina, 15-inch, Mid 2015)
- OS:  macOS Sierra 10.12.6
- CPU:  2.2 GHz Intel Core i7
- RAM:  16 GB 1600 MHz DDR3

### Notes

- For efficiency, I have chosen to implement the hash table in the PCY algorithm using a `std::vector`. This is because the
hash functions used yield integers, in the range of 0 to (NUM_BUCKETS-1). Therefore, the hash values are more efficiently
used as indices to any indexable array, achieving O(1) lookup. *It's functionality here is identical to any hash table whose keys
are unsigned integers.*
- Frequent items are stored in `std::unordered_set`, so that O(1) lookup efficiency is achieved.
- In-between pass operations following a pass are performed in the respective function for that pass. This way, the hash tables,
for example, can be deallocated from the stack when the function loses scope. This makes sufficient room for the bitmap(s)
and hash tables on subsequent passes.
